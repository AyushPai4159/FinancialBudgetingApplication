{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877d61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2\", temperature=0, system=\"you are a good bot\")\n",
    "\n",
    "print(llm.invoke(\"Tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6210a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nHope that made you smile! Do you want to hear another one?\"]\n"
     ]
    }
   ],
   "source": [
    "print(llm.batch([\"Tell me a joke\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c8f0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "What\n",
      " do\n",
      " you\n",
      " call\n",
      " a\n",
      " fake\n",
      " nood\n",
      "le\n",
      "?\n",
      "\n",
      "\n",
      "An\n",
      " imp\n",
      "asta\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " made\n",
      " you\n",
      " smile\n",
      "!\n",
      " Do\n",
      " you\n",
      " want\n",
      " to\n",
      " hear\n",
      " another\n",
      " one\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in llm.stream(\"Tell me a joke\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffd69634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "What do you call a fake noodle?\n",
      "\n",
      "An impasta!\n",
      "\n",
      "Hope that made you smile! Do you want to hear another one?\n",
      "[\"Here's one:\\n\\nWhat do you call a fake noodle?\\n\\nAn impasta!\\n\\nHope that made you smile! Do you want to hear another one?\"]\n",
      "Here\n",
      "'s\n",
      " one\n",
      ":\n",
      "\n",
      "\n",
      "What\n",
      " do\n",
      " you\n",
      " call\n",
      " a\n",
      " fake\n",
      " nood\n",
      "le\n",
      "?\n",
      "\n",
      "\n",
      "An\n",
      " imp\n",
      "asta\n",
      "!\n",
      "\n",
      "\n",
      "Hope\n",
      " that\n",
      " made\n",
      " you\n",
      " smile\n",
      "!\n",
      " Do\n",
      " you\n",
      " want\n",
      " to\n",
      " hear\n",
      " another\n",
      " one\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(await llm.ainvoke(\"Tell me a joke\"))\n",
    "print(await llm.abatch([\"Tell me a joke\"]))\n",
    "async for chunk in llm.astream(\"Tell me a joke\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f34fe8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "A man walked into a library and asked the librarian, \"Do you have any books on Pavlov's dogs and Schr√∂dinger's cat?\"\n",
      "\n",
      "The librarian replied, \"It rings a bell, but I'm not sure if it's here or not.\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke([{\"content\": \"Tell me a joke\", \"type\": \"user\"}]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3fa4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c2/2gwnjy5n3y31l8bn2qzc75wm0000gn/T/ipykernel_1799/1634378388.py:4: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  chat_llm = ChatOllama(model=\"llama3.2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Why don't eggs tell jokes?\\n\\nBecause they'd crack each other up!\" additional_kwargs={} response_metadata={'model': 'llama3.2', 'created_at': '2025-01-02T17:28:24.390334Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 346638625, 'load_duration': 11792167, 'prompt_eval_count': 29, 'prompt_eval_duration': 59000000, 'eval_count': 16, 'eval_duration': 274000000} id='run-e87466b1-581b-416a-9899-9e851a6468ab-0'\n"
     ]
    }
   ],
   "source": [
    "#importing chat_models instead of llms\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "chat_llm = ChatOllama(model=\"llama3.2\")\n",
    "print(chat_llm.invoke(\"tell me a joke\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69ec6f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"content\":\"foo\",\"additional_kwargs\":{},\"response_metadata\":{},\"type\":\"human\",\"name\":null,\"id\":null}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "print(BaseMessage(content=\"foo\", type=\"human\").json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e45cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "print(HumanMessage(content=\"Hello there\").type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2c2a3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='You are an expert in kite surfing, and your name is Steve.' additional_kwargs={} response_metadata={}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import SystemMessagePromptTemplate\n",
    "\n",
    "# create the MessagePromptTemplate\n",
    "\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are an expert in {subject}, and your name is {name}.\"\n",
    ")\n",
    "\n",
    "system_prompt = system_prompt_template.format(\n",
    "    subject=\"kite surfing\",\n",
    "    name=\"Steve\"\n",
    ")\n",
    "\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c8260fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful AI and your name is Bob\n",
      "Human: Hello, how are you doing\n",
      "AI: I'm doing well, thanks!\n",
      "Human: What is your name?\n"
     ]
    }
   ],
   "source": [
    "#passing variables into prompts and other messages\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "system_prompt_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful AI and your name is {name}\"\n",
    ")\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    system_prompt_template,\n",
    "    HumanMessage(\"Hello, how are you doing\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    \"{user_input}\"\n",
    "])\n",
    "\n",
    "prompt_value = template.format(\n",
    "\n",
    "    name=\"Bob\",\n",
    "    user_input=\"What is your name?\"\n",
    ")\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "400f59e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful AI and your name is Steve', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hello, how are you doing', additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm doing well, thanks!\", additional_kwargs={}, response_metadata={}), HumanMessage(content='What is your name?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt_value = template.invoke(\n",
    "\n",
    "    {\n",
    "        \"name\": \"Steve\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prompt_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775b5e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say hello\n",
      "Say hello\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Say {foo}\")\n",
    "print(prompt.format(foo=\"hello\")) #or invoke - it's a Runnable!\n",
    "\n",
    "# This is the same as the native python implementation\n",
    "prompt = \"Say {foo}\"\n",
    "print(prompt.format(foo=\"hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744a23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
